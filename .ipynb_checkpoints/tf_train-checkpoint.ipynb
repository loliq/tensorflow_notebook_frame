{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Package\n",
    "- 问题点，在jupyter使用tf调试代码时候记得要清空缓存，重新开始，不然似乎会出现网络重定义的行为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import tensorflow.contrib.slim as slim\n",
    "from create_record_files import get_example_nums\n",
    "import PARAMS as Param\n",
    "from models import dense_net as net\n",
    "import dataset_factory.dataset_factory as datasets\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. define train -parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = Param.Params()\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 512\n",
    "NUM_CLASS = PARAMS.params['model']['classNum']\n",
    "WEIGHT_DECAY = 1e-5\n",
    "KEEP_PROB = 0.6\n",
    "BASE_LR1 = 1e-3\n",
    "BASE_LR2 = 5e-4\n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_WIDTH = 224\n",
    "IMAGE_DEPTH = 3\n",
    "DECAY_EPOCH = 8  # learning_rate 衰减的时间\n",
    "TENSORBOARD_PATH = PARAMS.params['path']['train_tensorBoardPath']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. define file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/iwatch_224_record/train\n",
      "['train-00-of-00']\n",
      "WARNING:tensorflow:From E:\\01-jupyter\\01-tf-train\\create_record_files.py:68: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "train num = 280\n",
      "['val-00-of-00']\n",
      "validation num = 70\n"
     ]
    }
   ],
   "source": [
    "train_dir = os.path.dirname(PARAMS.params['path']['train_rex'])\n",
    "print(train_dir)\n",
    "train_num = get_example_nums(train_dir)\n",
    "print(\"train num = {0}\".format(train_num))\n",
    "val_dir = os.path.dirname(PARAMS.params['path']['val_rex'])\n",
    "val_num = get_example_nums(val_dir)\n",
    "print(\"validation num = {0}\".format(val_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset/iwatch_224_record/train\\\\train-00-of-00']\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "['dataset/iwatch_224_record/val\\\\val-00-of-00']\n"
     ]
    }
   ],
   "source": [
    "# print_tensors_in_checkpoint_file(MODEL_PATH, None, False, True)\n",
    "#定义数据集\n",
    "training_dataset = datasets.get_dataset('iwatch', train_dir,\n",
    "                                        'train', batch_size=BATCH_SIZE)\n",
    "# train_dataset 用epochs控制循环\n",
    "validation_dataset = datasets.get_dataset('iwatch', val_dir,\n",
    "                                        'test', batch_size=BATCH_SIZE)\n",
    "\n",
    "train_iterator = training_dataset.make_initializable_iterator()\n",
    "# make_initializable_iterator 每个epoch都需要初始化\n",
    "val_iterator = validation_dataset.make_initializable_iterator()\n",
    "# make_one_shot_iterator不需要初始化，根据需要不停循环\n",
    "train_images, train_labels = train_iterator.get_next()\n",
    "val_images, val_labels = val_iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. define models\n",
    " - define input placehoder\n",
    " - define propagate\n",
    " - define loss-function\n",
    " - define train_op\n",
    " - plot feature-map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py:1067: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# 定义输入\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    images = tf.placeholder(\n",
    "        dtype=tf.float32, shape=[None, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH], name='inputs')\n",
    "    labels = tf.placeholder(dtype=tf.int32, shape=[None, NUM_CLASS], name='label')\n",
    "\n",
    "with tf.name_scope('nets'):\n",
    "    logits, endPoints = net.inference(inputs=images, num_classes=NUM_CLASS,\n",
    "                                     is_training=is_training, dropout_keep_prob = KEEP_PROB)\n",
    "\n",
    "with tf.name_scope(\"loss_function\"):\n",
    "    tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)\n",
    "    loss_entro = tf.losses.get_losses()\n",
    "    loss_entro_mean = tf.reduce_mean(loss_entro)\n",
    "    tf.summary.scalar(\"cross_entropy\", loss_entro_mean)\n",
    "# prediction\n",
    "\n",
    "with tf.name_scope(\"train_op\"):\n",
    "    # fc8_optimizer = tf.train.GradientDescentOptimizer(BASE_LR1)  不能定义太多优化器，内存会爆掉\n",
    "    global_step = tf.train.create_global_step()\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        PARAMS.params['model']['baseLR'],\n",
    "        global_step,\n",
    "        DECAY_EPOCH*train_num / BATCH_SIZE,\n",
    "        PARAMS.params['model']['decayLR'])\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "    full_train_op = slim.learning.create_train_op(loss_entro_mean,\n",
    "                                                  optimizer,\n",
    "                                                  global_step=global_step)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "#         print(\"BN parameters: \", update_ops)\n",
    "        updates = tf.group(*update_ops)\n",
    "        full_train_op = control_flow_ops.with_dependencies([updates], full_train_op)\n",
    "#     for v in tf.all_variables():\n",
    "#         print(v.name)\n",
    "#         if 'batch_normalization' in v.name:\n",
    "#             tf.summary.histogram(v.name, v)\n",
    "\n",
    "    # 冻结fc7 以前的层\n",
    "    # fc8_train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step, var_list=fc8_variables)\n",
    "    # 全部训练\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    prediction = tf.argmax(logits, 1)\n",
    "    correct_prediction = tf.equal(prediction, tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "with tf.name_scope(\"input\"):\n",
    "    image_shape_input = tf.reshape(images, [-1, 224, 224, 1])\n",
    "    tf.summary.image('input_image', image_shape_input, 10)\n",
    "\n",
    "with tf.name_scope(\"feature_map\"):\n",
    "    feture_maps = endPoints[\"layer1\"]\n",
    "    # 取其中的第一张图像\n",
    "    # TODO 第一层输出的feauture值可能会不一样\n",
    "    # TODO ix，iy为宽度和高度\n",
    "    # TODO cx cy为每行显示的图像 ix*iy=channels\n",
    "    feture_maps = tf.slice(feture_maps, (0, 0, 0, 0), (1, -1, -1, -1))\n",
    "    ix = iy = 112\n",
    "    ix += 4\n",
    "    iy += 4\n",
    "    #类似subplot的方法图像显示为4x4\n",
    "    cy = 4\n",
    "    cx = 3\n",
    "    #做padding方便将所有图分开\n",
    "    feture_maps = tf.image.resize_image_with_crop_or_pad(feture_maps, iy, ix)\n",
    "    # reshape 成 56x56x4x4\n",
    "    feture_maps = tf.reshape(feture_maps, (iy, ix, cy, cx))\n",
    "    #交换维度\n",
    "    feture_maps = tf.transpose(feture_maps, (2, 0, 3, 1))  # cy,iy,cx\n",
    "    #将所有通道的图片组成一张图片显示\n",
    "    feture_maps = tf.reshape(feture_maps, (1, cy * iy, cx * ix, 1))\n",
    "\n",
    "    tf.summary.image(\"layer1_feature_map\", feture_maps, 1)\n",
    "\n",
    "init_op = [tf.global_variables_initializer(), tf.local_variables_initializer()]\n",
    "saver = tf.train.Saver()\n",
    "summary_merge = tf.summary.merge_all()\n",
    "\n",
    "# 结束当前的计算图，使之成为只读\n",
    "#     tf.get_default_graph().finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    acc_list = []\n",
    "    with tf.Session() as sess:\n",
    "        # 先初始化网络\n",
    "        sess.run(init_op)\n",
    "        tensorboard_writer = tf.summary.FileWriter(TENSORBOARD_PATH, sess.graph)\n",
    "        max_acc = 0.70\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            print('Starting epoch %d / %d' % (epoch, NUM_EPOCHS))\n",
    "            sess.run(train_iterator.initializer)\n",
    "            while True:\n",
    "                try:\n",
    "                    train_batch_images, train_batch_labels \\\n",
    "                        = sess.run([train_images, train_labels])\n",
    "                    _,summary, train_loss, train_acc = sess.run([full_train_op, summary_merge,loss_entro_mean, accuracy],\n",
    "                                                         feed_dict={is_training: True,\n",
    "                                                                images: train_batch_images,\n",
    "                                                                labels: train_batch_labels})\n",
    "                    step = sess.run(global_step)\n",
    "                    tensorboard_writer.add_summary(summary,step)\n",
    "                    if step %10 == 0:\n",
    "                        print(\"epoch = {0} step = {3}: train-loss = {1}, batch-acc = {2}\".format(epoch, train_loss,train_acc,step))\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "            sess.run(val_iterator.initializer)\n",
    "            num_correct = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    val_batch_images, val_batch_labels \\\n",
    "                        = sess.run([val_images, val_labels])\n",
    "\n",
    "                    correct_pred = sess.run(correct_prediction, feed_dict={\n",
    "                                                        is_training: False,\n",
    "                                                        images: val_batch_images,\n",
    "                                                        labels: val_batch_labels})\n",
    "\n",
    "                    num_correct += correct_pred.sum()\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "            val_acc = float(num_correct) / val_num\n",
    "            acc_list.append(val_acc)\n",
    "            # Plot learning curve (with costs)\n",
    "            if val_acc > max_acc or epoch > NUM_EPOCHS-2:\n",
    "                max_acc = val_acc\n",
    "                best_models = os.path.join(PARAMS.params['path']['model_path'],\n",
    "                                           'model_epoch{}_{:.4f}.ckpt'.format(epoch, val_acc))\n",
    "                saver.save(sess,best_models)\n",
    "            print(\"epoch = {0}, val-acc = {1}\".format(epoch, val_acc))\n",
    "        tensorboard_writer.close()\n",
    "        return acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0 / 512\n",
      "epoch = 0, val-acc = 0.5142857142857142\n",
      "Starting epoch 1 / 512\n",
      "epoch = 1 step = 10: train-loss = 0.8099629282951355, batch-acc = 0.5\n",
      "epoch = 1, val-acc = 0.5142857142857142\n",
      "Starting epoch 2 / 512\n",
      "epoch = 2 step = 20: train-loss = 0.7646796703338623, batch-acc = 0.4375\n",
      "epoch = 2, val-acc = 0.5142857142857142\n",
      "Starting epoch 3 / 512\n",
      "epoch = 3 step = 30: train-loss = 0.7609186172485352, batch-acc = 0.46875\n",
      "epoch = 3, val-acc = 0.5142857142857142\n",
      "Starting epoch 4 / 512\n",
      "epoch = 4 step = 40: train-loss = 0.7783623933792114, batch-acc = 0.40625\n",
      "epoch = 4, val-acc = 0.5142857142857142\n",
      "Starting epoch 5 / 512\n",
      "epoch = 5 step = 50: train-loss = 0.6874964237213135, batch-acc = 0.53125\n",
      "epoch = 5, val-acc = 0.5142857142857142\n",
      "Starting epoch 6 / 512\n",
      "epoch = 6 step = 60: train-loss = 0.6752865314483643, batch-acc = 0.6875\n",
      "epoch = 6, val-acc = 0.5285714285714286\n",
      "Starting epoch 7 / 512\n",
      "epoch = 7 step = 70: train-loss = 0.7265956997871399, batch-acc = 0.4375\n",
      "epoch = 7, val-acc = 0.5285714285714286\n",
      "Starting epoch 8 / 512\n",
      "epoch = 8 step = 80: train-loss = 0.6579656600952148, batch-acc = 0.5625\n",
      "epoch = 8, val-acc = 0.5142857142857142\n",
      "Starting epoch 9 / 512\n",
      "epoch = 9 step = 90: train-loss = 0.7428882718086243, batch-acc = 0.4583333432674408\n",
      "epoch = 9, val-acc = 0.6142857142857143\n",
      "Starting epoch 10 / 512\n",
      "epoch = 10, val-acc = 0.6285714285714286\n",
      "Starting epoch 11 / 512\n",
      "epoch = 11 step = 100: train-loss = 0.7560533881187439, batch-acc = 0.46875\n",
      "epoch = 11, val-acc = 0.5857142857142857\n",
      "Starting epoch 12 / 512\n",
      "epoch = 12 step = 110: train-loss = 0.6966710090637207, batch-acc = 0.53125\n",
      "epoch = 12, val-acc = 0.5714285714285714\n",
      "Starting epoch 13 / 512\n",
      "epoch = 13 step = 120: train-loss = 0.7312213182449341, batch-acc = 0.46875\n",
      "epoch = 13, val-acc = 0.6428571428571429\n",
      "Starting epoch 14 / 512\n",
      "epoch = 14 step = 130: train-loss = 0.6893910765647888, batch-acc = 0.5625\n",
      "epoch = 14, val-acc = 0.5857142857142857\n",
      "Starting epoch 15 / 512\n",
      "epoch = 15 step = 140: train-loss = 0.7116712331771851, batch-acc = 0.5625\n",
      "epoch = 15, val-acc = 0.6142857142857143\n",
      "Starting epoch 16 / 512\n",
      "epoch = 16 step = 150: train-loss = 0.7169417142868042, batch-acc = 0.40625\n",
      "epoch = 16, val-acc = 0.5285714285714286\n",
      "Starting epoch 17 / 512\n",
      "epoch = 17 step = 160: train-loss = 0.6778149008750916, batch-acc = 0.65625\n",
      "epoch = 17, val-acc = 0.5142857142857142\n",
      "Starting epoch 18 / 512\n",
      "epoch = 18 step = 170: train-loss = 0.6508013606071472, batch-acc = 0.625\n",
      "epoch = 18, val-acc = 0.5285714285714286\n",
      "Starting epoch 19 / 512\n",
      "epoch = 19 step = 180: train-loss = 0.7011873126029968, batch-acc = 0.5\n",
      "epoch = 19, val-acc = 0.4857142857142857\n",
      "Starting epoch 20 / 512\n",
      "epoch = 20, val-acc = 0.5\n",
      "Starting epoch 21 / 512\n",
      "epoch = 21 step = 190: train-loss = 0.6841304302215576, batch-acc = 0.4375\n",
      "epoch = 21, val-acc = 0.5\n",
      "Starting epoch 22 / 512\n",
      "epoch = 22 step = 200: train-loss = 0.6409796476364136, batch-acc = 0.71875\n",
      "epoch = 22, val-acc = 0.5857142857142857\n",
      "Starting epoch 23 / 512\n",
      "epoch = 23 step = 210: train-loss = 0.7084802389144897, batch-acc = 0.46875\n",
      "epoch = 23, val-acc = 0.5857142857142857\n",
      "Starting epoch 24 / 512\n",
      "epoch = 24 step = 220: train-loss = 0.6783474683761597, batch-acc = 0.40625\n",
      "epoch = 24, val-acc = 0.6142857142857143\n",
      "Starting epoch 25 / 512\n",
      "epoch = 25 step = 230: train-loss = 0.7261825203895569, batch-acc = 0.40625\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    acc_list = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    plt.plot(acc_list)\n",
    "    plt.ylabel('acc')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.title(\"model = {0}, base_lr = {1}, batch_size = {2}\".format(\"senet_3\", BASE_LR1, BATCH_SIZE))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
