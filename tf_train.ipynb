{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Package\n",
    "- 问题点，在jupyter使用tf调试代码时候记得要清空缓存，重新开始，不然似乎会出现网络重定义的行为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import tensorflow.contrib.slim as slim\n",
    "from create_record_files import get_example_nums\n",
    "import PARAMS as Param\n",
    "from models import dense_net_b3_k12_se as net\n",
    "import dataset_factory.dataset_factory as datasets\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. define train -parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = Param.Params()\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 300\n",
    "NUM_CLASS = 3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "KEEP_PROB = 0.9\n",
    "BASE_LR1 = 1e-3\n",
    "BASE_LR2 = 5e-4\n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_WIDTH = 224\n",
    "IMAGE_DEPTH = 3\n",
    "DECAY_EPOCH = 8  # learning_rate 衰减的时间\n",
    "TENSORBOARD_PATH = PARAMS.params['path']['train_tensorBoardPath']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. define file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/record_file/iwatch_224_record_v5/train\n",
      "['train-00-of-00']\n",
      "WARNING:tensorflow:From E:\\01-jupyter\\01-tf-train\\create_record_files.py:68: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "train num = 439\n",
      "['val-00-of-00']\n",
      "validation num = 110\n"
     ]
    }
   ],
   "source": [
    "validation_index = 5\n",
    "train_dir = \"dataset/record_file/iwatch_224_record_v{0}/train\".format(validation_index)\n",
    "print(train_dir)\n",
    "train_num = get_example_nums(train_dir)\n",
    "print(\"train num = {0}\".format(train_num))\n",
    "val_dir = \"dataset/record_file/iwatch_224_record_v{0}/val\".format(validation_index)\n",
    "val_num = get_example_nums(val_dir)\n",
    "print(\"validation num = {0}\".format(val_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset/record_file/iwatch_224_record_v5/train\\\\train-00-of-00']\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "['dataset/record_file/iwatch_224_record_v5/val\\\\val-00-of-00']\n"
     ]
    }
   ],
   "source": [
    "# print_tensors_in_checkpoint_file(MODEL_PATH, None, False, True)\n",
    "#定义数据集\n",
    "training_dataset = datasets.get_dataset('iwatch', train_dir,\n",
    "                                        'train', batch_size=BATCH_SIZE,label_num=3)\n",
    "# train_dataset 用epochs控制循环\n",
    "validation_dataset = datasets.get_dataset('iwatch', val_dir,\n",
    "                                        'test', batch_size=BATCH_SIZE, label_num=3)\n",
    "\n",
    "train_iterator = training_dataset.make_initializable_iterator()\n",
    "# make_initializable_iterator 每个epoch都需要初始化\n",
    "val_iterator = validation_dataset.make_initializable_iterator()\n",
    "# make_one_shot_iterator不需要初始化，根据需要不停循环\n",
    "train_images, train_labels = train_iterator.get_next()\n",
    "val_images, val_labels = val_iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. define models\n",
    " - define input placehoder\n",
    " - define propagate\n",
    " - define loss-function\n",
    " - define train_op\n",
    " - plot feature-map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py:1067: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# 定义输入\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    images = tf.placeholder(\n",
    "        dtype=tf.float32, shape=[None, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH], name='inputs')\n",
    "    labels = tf.placeholder(dtype=tf.int32, shape=[None, NUM_CLASS], name='label')\n",
    "\n",
    "with tf.name_scope('nets'):\n",
    "    logits, endPoints = net.inference(inputs=images, num_classes=NUM_CLASS,\n",
    "                                     is_training=is_training, dropout_keep_prob = KEEP_PROB)\n",
    "\n",
    "with tf.name_scope(\"loss_function\"):\n",
    "    tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)\n",
    "    loss_entro = tf.losses.get_losses()\n",
    "    loss_entro_mean = tf.reduce_mean(loss_entro)\n",
    "    tf.summary.scalar(\"cross_entropy\", loss_entro_mean)\n",
    "# prediction\n",
    "\n",
    "with tf.name_scope(\"train_op\"):\n",
    "    # fc8_optimizer = tf.train.GradientDescentOptimizer(BASE_LR1)  不能定义太多优化器，内存会爆掉\n",
    "    global_step = tf.train.create_global_step()\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        PARAMS.params['model']['baseLR'],\n",
    "        global_step,\n",
    "        DECAY_EPOCH*train_num / BATCH_SIZE,\n",
    "        PARAMS.params['model']['decayLR'])\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "    full_train_op = slim.learning.create_train_op(loss_entro_mean,\n",
    "                                                  optimizer,\n",
    "                                                  global_step=global_step)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "#         print(\"BN parameters: \", update_ops)\n",
    "        updates = tf.group(*update_ops)\n",
    "        full_train_op = control_flow_ops.with_dependencies([updates], full_train_op)\n",
    "#     for v in tf.all_variables():\n",
    "#         print(v.name)\n",
    "#         if 'batch_normalization' in v.name:\n",
    "#             tf.summary.histogram(v.name, v)\n",
    "\n",
    "    # 冻结fc7 以前的层\n",
    "    # fc8_train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step, var_list=fc8_variables)\n",
    "    # 全部训练\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    prediction = tf.argmax(logits, 1)\n",
    "    correct_prediction = tf.equal(prediction, tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "with tf.name_scope(\"input\"):\n",
    "    image_shape_input = tf.reshape(images, [-1, 224, 224, 1])\n",
    "    tf.summary.image('input_image', image_shape_input, 10)\n",
    "\n",
    "with tf.name_scope(\"feature_map\"):\n",
    "    feture_maps = endPoints[\"layer1\"]\n",
    "    # 取其中的第一张图像\n",
    "    # TODO 第一层输出的feauture值可能会不一样\n",
    "    # TODO ix，iy为宽度和高度\n",
    "    # TODO cx cy为每行显示的图像 ix*iy=channels\n",
    "    feture_maps = tf.slice(feture_maps, (0, 0, 0, 0), (1, -1, -1, -1))\n",
    "    ix = iy = 112\n",
    "    ix += 4\n",
    "    iy += 4\n",
    "    #类似subplot的方法图像显示为4x4\n",
    "    cy = 4\n",
    "    cx = 3\n",
    "    #做padding方便将所有图分开\n",
    "    feture_maps = tf.image.resize_image_with_crop_or_pad(feture_maps, iy, ix)\n",
    "    # reshape 成 56x56x4x4\n",
    "    feture_maps = tf.reshape(feture_maps, (iy, ix, cy, cx))\n",
    "    #交换维度\n",
    "    feture_maps = tf.transpose(feture_maps, (2, 0, 3, 1))  # cy,iy,cx\n",
    "    #将所有通道的图片组成一张图片显示\n",
    "    feture_maps = tf.reshape(feture_maps, (1, cy * iy, cx * ix, 1))\n",
    "\n",
    "    tf.summary.image(\"layer1_feature_map\", feture_maps, 1)\n",
    "\n",
    "init_op = [tf.global_variables_initializer(), tf.local_variables_initializer()]\n",
    "saver = tf.train.Saver()\n",
    "summary_merge = tf.summary.merge_all()\n",
    "\n",
    "# 结束当前的计算图，使之成为只读\n",
    "#     tf.get_default_graph().finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    acc_list = []\n",
    "    with tf.Session() as sess:\n",
    "        # 先初始化网络\n",
    "        sess.run(init_op)\n",
    "        tensorboard_writer = tf.summary.FileWriter(TENSORBOARD_PATH, sess.graph)\n",
    "        max_acc = 0.70\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            print('Starting epoch %d / %d' % (epoch, NUM_EPOCHS))\n",
    "            sess.run(train_iterator.initializer)\n",
    "            while True:\n",
    "                try:\n",
    "                    train_batch_images, train_batch_labels \\\n",
    "                        = sess.run([train_images, train_labels])\n",
    "                    _,summary, train_loss, train_acc = sess.run([full_train_op, summary_merge,loss_entro_mean, accuracy],\n",
    "                                                         feed_dict={is_training: True,\n",
    "                                                                images: train_batch_images,\n",
    "                                                                labels: train_batch_labels})\n",
    "                    step = sess.run(global_step)\n",
    "                    tensorboard_writer.add_summary(summary,step)\n",
    "                    if step %10 == 0:\n",
    "                        print(\"epoch = {0} step = {3}: train-loss = {1}, batch-acc = {2}\".format(epoch, train_loss,train_acc,step))\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "            sess.run(val_iterator.initializer)\n",
    "            num_correct = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    val_batch_images, val_batch_labels \\\n",
    "                        = sess.run([val_images, val_labels])\n",
    "\n",
    "                    correct_pred = sess.run(correct_prediction, feed_dict={\n",
    "                                                        is_training: False,\n",
    "                                                        images: val_batch_images,\n",
    "                                                        labels: val_batch_labels})\n",
    "\n",
    "                    num_correct += correct_pred.sum()\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "            val_acc = float(num_correct) / val_num\n",
    "            acc_list.append(val_acc)\n",
    "            # Plot learning curve (with costs)\n",
    "            if val_acc >= max_acc or epoch > NUM_EPOCHS-2:\n",
    "                max_acc = val_acc\n",
    "                best_models = os.path.join(PARAMS.params['path']['model_path'],\n",
    "                                           'model_epoch{}_{:.4f}.ckpt'.format(epoch, val_acc))\n",
    "                saver.save(sess,best_models)\n",
    "            print(\"epoch = {0}, val-acc = {1}\".format(epoch, val_acc))\n",
    "        tensorboard_writer.close()\n",
    "        return acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0 / 300\n",
      "epoch = 0 step = 10: train-loss = 1.019791603088379, batch-acc = 0.40625\n",
      "epoch = 0, val-acc = 0.2818181818181818\n",
      "Starting epoch 1 / 300\n",
      "epoch = 1 step = 20: train-loss = 1.0328987836837769, batch-acc = 0.34375\n",
      "epoch = 1, val-acc = 0.2818181818181818\n",
      "Starting epoch 2 / 300\n",
      "epoch = 2 step = 30: train-loss = 1.0051724910736084, batch-acc = 0.46875\n",
      "epoch = 2 step = 40: train-loss = 1.117214560508728, batch-acc = 0.3125\n",
      "epoch = 2, val-acc = 0.2818181818181818\n",
      "Starting epoch 3 / 300\n",
      "epoch = 3 step = 50: train-loss = 1.0331687927246094, batch-acc = 0.46875\n",
      "epoch = 3, val-acc = 0.2818181818181818\n",
      "Starting epoch 4 / 300\n",
      "epoch = 4 step = 60: train-loss = 0.8802303075790405, batch-acc = 0.65625\n",
      "epoch = 4 step = 70: train-loss = 0.9815853238105774, batch-acc = 0.52173912525177\n",
      "epoch = 4, val-acc = 0.2818181818181818\n",
      "Starting epoch 5 / 300\n",
      "epoch = 5 step = 80: train-loss = 0.8540412783622742, batch-acc = 0.59375\n",
      "epoch = 5, val-acc = 0.2818181818181818\n",
      "Starting epoch 6 / 300\n",
      "epoch = 6 step = 90: train-loss = 0.7701340317726135, batch-acc = 0.625\n",
      "epoch = 6, val-acc = 0.2909090909090909\n",
      "Starting epoch 7 / 300\n",
      "epoch = 7 step = 100: train-loss = 0.8041865229606628, batch-acc = 0.59375\n",
      "epoch = 7 step = 110: train-loss = 0.627023458480835, batch-acc = 0.84375\n",
      "epoch = 7, val-acc = 0.35454545454545455\n",
      "Starting epoch 8 / 300\n",
      "epoch = 8 step = 120: train-loss = 0.9625188708305359, batch-acc = 0.5\n",
      "epoch = 8, val-acc = 0.4909090909090909\n",
      "Starting epoch 9 / 300\n",
      "epoch = 9 step = 130: train-loss = 0.8076046705245972, batch-acc = 0.65625\n",
      "epoch = 9 step = 140: train-loss = 0.7492359280586243, batch-acc = 0.739130437374115\n",
      "epoch = 9, val-acc = 0.39090909090909093\n",
      "Starting epoch 10 / 300\n",
      "epoch = 10 step = 150: train-loss = 0.8942002058029175, batch-acc = 0.5625\n",
      "epoch = 10, val-acc = 0.5363636363636364\n",
      "Starting epoch 11 / 300\n",
      "epoch = 11 step = 160: train-loss = 0.692465603351593, batch-acc = 0.75\n",
      "epoch = 11, val-acc = 0.5363636363636364\n",
      "Starting epoch 12 / 300\n",
      "epoch = 12 step = 170: train-loss = 0.7763900756835938, batch-acc = 0.71875\n",
      "epoch = 12 step = 180: train-loss = 0.6920456886291504, batch-acc = 0.71875\n",
      "epoch = 12, val-acc = 0.6090909090909091\n",
      "Starting epoch 13 / 300\n",
      "epoch = 13 step = 190: train-loss = 0.6897619366645813, batch-acc = 0.78125\n",
      "epoch = 13, val-acc = 0.4909090909090909\n",
      "Starting epoch 14 / 300\n",
      "epoch = 14 step = 200: train-loss = 0.6775082945823669, batch-acc = 0.75\n",
      "epoch = 14 step = 210: train-loss = 0.6907702088356018, batch-acc = 0.782608687877655\n",
      "epoch = 14, val-acc = 0.5\n",
      "Starting epoch 15 / 300\n",
      "epoch = 15 step = 220: train-loss = 0.6028344631195068, batch-acc = 0.84375\n",
      "epoch = 15, val-acc = 0.5181818181818182\n",
      "Starting epoch 16 / 300\n",
      "epoch = 16 step = 230: train-loss = 0.6627238988876343, batch-acc = 0.71875\n",
      "epoch = 16, val-acc = 0.45454545454545453\n",
      "Starting epoch 17 / 300\n",
      "epoch = 17 step = 240: train-loss = 0.6320556402206421, batch-acc = 0.71875\n",
      "epoch = 17 step = 250: train-loss = 0.7951233386993408, batch-acc = 0.625\n",
      "epoch = 17, val-acc = 0.5545454545454546\n",
      "Starting epoch 18 / 300\n",
      "epoch = 18 step = 260: train-loss = 0.5819602012634277, batch-acc = 0.78125\n",
      "epoch = 18, val-acc = 0.5909090909090909\n",
      "Starting epoch 19 / 300\n",
      "epoch = 19 step = 270: train-loss = 0.6695306301116943, batch-acc = 0.6875\n",
      "epoch = 19 step = 280: train-loss = 0.5446325540542603, batch-acc = 0.8260869383811951\n",
      "epoch = 19, val-acc = 0.7\n",
      "Starting epoch 20 / 300\n",
      "epoch = 20 step = 290: train-loss = 0.5053771734237671, batch-acc = 0.875\n",
      "epoch = 20, val-acc = 0.5454545454545454\n",
      "Starting epoch 21 / 300\n",
      "epoch = 21 step = 300: train-loss = 0.49499377608299255, batch-acc = 0.8125\n",
      "epoch = 21, val-acc = 0.5\n",
      "Starting epoch 22 / 300\n",
      "epoch = 22 step = 310: train-loss = 0.6195806264877319, batch-acc = 0.78125\n",
      "epoch = 22 step = 320: train-loss = 0.5633039474487305, batch-acc = 0.8125\n",
      "epoch = 22, val-acc = 0.6454545454545455\n",
      "Starting epoch 23 / 300\n",
      "epoch = 23 step = 330: train-loss = 0.5234115719795227, batch-acc = 0.78125\n",
      "epoch = 23, val-acc = 0.5909090909090909\n",
      "Starting epoch 24 / 300\n",
      "epoch = 24 step = 340: train-loss = 0.5850167274475098, batch-acc = 0.71875\n",
      "epoch = 24 step = 350: train-loss = 0.5354865789413452, batch-acc = 0.782608687877655\n",
      "epoch = 24, val-acc = 0.6818181818181818\n",
      "Starting epoch 25 / 300\n",
      "epoch = 25 step = 360: train-loss = 0.6008024215698242, batch-acc = 0.78125\n",
      "epoch = 25, val-acc = 0.6818181818181818\n",
      "Starting epoch 26 / 300\n",
      "epoch = 26 step = 370: train-loss = 0.49955588579177856, batch-acc = 0.75\n",
      "epoch = 26, val-acc = 0.7454545454545455\n",
      "Starting epoch 27 / 300\n",
      "epoch = 27 step = 380: train-loss = 0.39372512698173523, batch-acc = 0.96875\n",
      "epoch = 27 step = 390: train-loss = 0.5046707391738892, batch-acc = 0.8125\n",
      "epoch = 27, val-acc = 0.7\n",
      "Starting epoch 28 / 300\n",
      "epoch = 28 step = 400: train-loss = 0.6656860113143921, batch-acc = 0.71875\n",
      "epoch = 28, val-acc = 0.6090909090909091\n",
      "Starting epoch 29 / 300\n",
      "epoch = 29 step = 410: train-loss = 0.5292053818702698, batch-acc = 0.8125\n",
      "epoch = 29 step = 420: train-loss = 0.7364457845687866, batch-acc = 0.695652186870575\n",
      "epoch = 29, val-acc = 0.6090909090909091\n",
      "Starting epoch 30 / 300\n",
      "epoch = 30 step = 430: train-loss = 0.4547269344329834, batch-acc = 0.84375\n",
      "epoch = 30, val-acc = 0.5818181818181818\n",
      "Starting epoch 31 / 300\n",
      "epoch = 31 step = 440: train-loss = 0.49674808979034424, batch-acc = 0.78125\n",
      "epoch = 31, val-acc = 0.7909090909090909\n",
      "Starting epoch 32 / 300\n",
      "epoch = 32 step = 450: train-loss = 0.4114936888217926, batch-acc = 0.90625\n",
      "epoch = 32 step = 460: train-loss = 0.5030471086502075, batch-acc = 0.84375\n",
      "epoch = 32, val-acc = 0.5545454545454546\n",
      "Starting epoch 33 / 300\n",
      "epoch = 33 step = 470: train-loss = 0.35461243987083435, batch-acc = 0.90625\n",
      "epoch = 33, val-acc = 0.6181818181818182\n",
      "Starting epoch 34 / 300\n",
      "epoch = 34 step = 480: train-loss = 0.3186874985694885, batch-acc = 0.9375\n",
      "epoch = 34 step = 490: train-loss = 0.5703542828559875, batch-acc = 0.8260869383811951\n",
      "epoch = 34, val-acc = 0.6909090909090909\n",
      "Starting epoch 35 / 300\n",
      "epoch = 35 step = 500: train-loss = 0.3497087359428406, batch-acc = 0.90625\n",
      "epoch = 35, val-acc = 0.6818181818181818\n",
      "Starting epoch 36 / 300\n",
      "epoch = 36 step = 510: train-loss = 0.527414858341217, batch-acc = 0.84375\n",
      "epoch = 36, val-acc = 0.7818181818181819\n",
      "Starting epoch 37 / 300\n",
      "epoch = 37 step = 520: train-loss = 0.31074678897857666, batch-acc = 0.9375\n",
      "epoch = 37 step = 530: train-loss = 0.3486660122871399, batch-acc = 0.875\n",
      "epoch = 37, val-acc = 0.7272727272727273\n",
      "Starting epoch 38 / 300\n",
      "epoch = 38 step = 540: train-loss = 0.38495543599128723, batch-acc = 0.875\n",
      "epoch = 38, val-acc = 0.8\n",
      "Starting epoch 39 / 300\n",
      "epoch = 39 step = 550: train-loss = 0.30125367641448975, batch-acc = 0.90625\n",
      "epoch = 39 step = 560: train-loss = 0.4946613311767578, batch-acc = 0.8695651888847351\n",
      "epoch = 39, val-acc = 0.7636363636363637\n",
      "Starting epoch 40 / 300\n",
      "epoch = 40 step = 570: train-loss = 0.24590536952018738, batch-acc = 0.96875\n",
      "epoch = 40, val-acc = 0.7636363636363637\n",
      "Starting epoch 41 / 300\n",
      "epoch = 41 step = 580: train-loss = 0.28522026538848877, batch-acc = 0.9375\n",
      "epoch = 41, val-acc = 0.7909090909090909\n",
      "Starting epoch 42 / 300\n",
      "epoch = 42 step = 590: train-loss = 0.24985358119010925, batch-acc = 0.96875\n",
      "epoch = 42 step = 600: train-loss = 0.29769089818000793, batch-acc = 0.9375\n",
      "epoch = 42, val-acc = 0.7727272727272727\n",
      "Starting epoch 43 / 300\n",
      "epoch = 43 step = 610: train-loss = 0.2898559868335724, batch-acc = 0.90625\n",
      "epoch = 43, val-acc = 0.8\n",
      "Starting epoch 44 / 300\n",
      "epoch = 44 step = 620: train-loss = 0.2544477581977844, batch-acc = 0.96875\n",
      "epoch = 44 step = 630: train-loss = 0.22809912264347076, batch-acc = 0.95652174949646\n",
      "epoch = 44, val-acc = 0.6272727272727273\n",
      "Starting epoch 45 / 300\n",
      "epoch = 45 step = 640: train-loss = 0.41956597566604614, batch-acc = 0.84375\n",
      "epoch = 45, val-acc = 0.6636363636363637\n",
      "Starting epoch 46 / 300\n",
      "epoch = 46 step = 650: train-loss = 0.2643629014492035, batch-acc = 0.9375\n",
      "epoch = 46, val-acc = 0.7181818181818181\n",
      "Starting epoch 47 / 300\n",
      "epoch = 47 step = 660: train-loss = 0.18932221829891205, batch-acc = 1.0\n",
      "epoch = 47 step = 670: train-loss = 0.4814918339252472, batch-acc = 0.84375\n",
      "epoch = 47, val-acc = 0.7909090909090909\n",
      "Starting epoch 48 / 300\n",
      "epoch = 48 step = 680: train-loss = 0.22453339397907257, batch-acc = 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 48, val-acc = 0.7636363636363637\n",
      "Starting epoch 49 / 300\n",
      "epoch = 49 step = 690: train-loss = 0.2524052560329437, batch-acc = 0.96875\n",
      "epoch = 49 step = 700: train-loss = 0.227411150932312, batch-acc = 0.95652174949646\n",
      "epoch = 49, val-acc = 0.7545454545454545\n",
      "Starting epoch 50 / 300\n",
      "epoch = 50 step = 710: train-loss = 0.19144755601882935, batch-acc = 1.0\n",
      "epoch = 50, val-acc = 0.7818181818181819\n",
      "Starting epoch 51 / 300\n",
      "epoch = 51 step = 720: train-loss = 0.17967887222766876, batch-acc = 1.0\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "epoch = 51, val-acc = 0.8090909090909091\n",
      "Starting epoch 52 / 300\n",
      "epoch = 52 step = 730: train-loss = 0.1879434734582901, batch-acc = 1.0\n",
      "epoch = 52 step = 740: train-loss = 0.2676052153110504, batch-acc = 0.9375\n",
      "epoch = 52, val-acc = 0.6818181818181818\n",
      "Starting epoch 53 / 300\n",
      "epoch = 53 step = 750: train-loss = 0.22257111966609955, batch-acc = 0.96875\n",
      "epoch = 53, val-acc = 0.7181818181818181\n",
      "Starting epoch 54 / 300\n",
      "epoch = 54 step = 760: train-loss = 0.28914445638656616, batch-acc = 0.875\n",
      "epoch = 54 step = 770: train-loss = 0.21991227567195892, batch-acc = 0.95652174949646\n",
      "epoch = 54, val-acc = 0.7454545454545455\n",
      "Starting epoch 55 / 300\n",
      "epoch = 55 step = 780: train-loss = 0.21477444469928741, batch-acc = 1.0\n",
      "epoch = 55, val-acc = 0.7545454545454545\n",
      "Starting epoch 56 / 300\n",
      "epoch = 56 step = 790: train-loss = 0.170362651348114, batch-acc = 1.0\n",
      "epoch = 56, val-acc = 0.8090909090909091\n",
      "Starting epoch 57 / 300\n",
      "epoch = 57 step = 800: train-loss = 0.2993903160095215, batch-acc = 0.875\n",
      "epoch = 57 step = 810: train-loss = 0.16358153522014618, batch-acc = 1.0\n",
      "epoch = 57, val-acc = 0.7545454545454545\n",
      "Starting epoch 58 / 300\n",
      "epoch = 58 step = 820: train-loss = 0.18011048436164856, batch-acc = 0.96875\n",
      "epoch = 58, val-acc = 0.8181818181818182\n",
      "Starting epoch 59 / 300\n",
      "epoch = 59 step = 830: train-loss = 0.1595994532108307, batch-acc = 1.0\n",
      "epoch = 59 step = 840: train-loss = 0.43646812438964844, batch-acc = 0.8695651888847351\n",
      "epoch = 59, val-acc = 0.8\n",
      "Starting epoch 60 / 300\n",
      "epoch = 60 step = 850: train-loss = 0.1780604124069214, batch-acc = 0.96875\n",
      "epoch = 60, val-acc = 0.7272727272727273\n",
      "Starting epoch 61 / 300\n",
      "epoch = 61 step = 860: train-loss = 0.16674989461898804, batch-acc = 0.96875\n",
      "epoch = 61, val-acc = 0.7454545454545455\n",
      "Starting epoch 62 / 300\n",
      "epoch = 62 step = 870: train-loss = 0.14904621243476868, batch-acc = 1.0\n",
      "epoch = 62 step = 880: train-loss = 0.16872796416282654, batch-acc = 0.96875\n",
      "epoch = 62, val-acc = 0.7818181818181819\n",
      "Starting epoch 63 / 300\n",
      "epoch = 63 step = 890: train-loss = 0.2010146677494049, batch-acc = 0.96875\n",
      "epoch = 63, val-acc = 0.7818181818181819\n",
      "Starting epoch 64 / 300\n",
      "epoch = 64 step = 900: train-loss = 0.13189628720283508, batch-acc = 1.0\n",
      "epoch = 64 step = 910: train-loss = 0.17508935928344727, batch-acc = 0.95652174949646\n",
      "epoch = 64, val-acc = 0.7818181818181819\n",
      "Starting epoch 65 / 300\n",
      "epoch = 65 step = 920: train-loss = 0.12845125794410706, batch-acc = 1.0\n",
      "epoch = 65, val-acc = 0.7181818181818181\n",
      "Starting epoch 66 / 300\n",
      "epoch = 66 step = 930: train-loss = 0.11946844309568405, batch-acc = 1.0\n",
      "epoch = 66, val-acc = 0.7909090909090909\n",
      "Starting epoch 67 / 300\n",
      "epoch = 67 step = 940: train-loss = 0.21393698453903198, batch-acc = 0.90625\n",
      "epoch = 67 step = 950: train-loss = 0.17681007087230682, batch-acc = 0.96875\n",
      "epoch = 67, val-acc = 0.7818181818181819\n",
      "Starting epoch 68 / 300\n",
      "epoch = 68 step = 960: train-loss = 0.13813787698745728, batch-acc = 1.0\n",
      "epoch = 68, val-acc = 0.5727272727272728\n",
      "Starting epoch 69 / 300\n",
      "epoch = 69 step = 970: train-loss = 0.1396661102771759, batch-acc = 1.0\n",
      "epoch = 69 step = 980: train-loss = 0.1951616108417511, batch-acc = 0.95652174949646\n",
      "epoch = 69, val-acc = 0.8090909090909091\n",
      "Starting epoch 70 / 300\n",
      "epoch = 70 step = 990: train-loss = 0.16711458563804626, batch-acc = 1.0\n",
      "epoch = 70, val-acc = 0.8090909090909091\n",
      "Starting epoch 71 / 300\n",
      "epoch = 71 step = 1000: train-loss = 0.17340083420276642, batch-acc = 0.96875\n",
      "epoch = 71, val-acc = 0.8363636363636363\n",
      "Starting epoch 72 / 300\n",
      "epoch = 72 step = 1010: train-loss = 0.1111648827791214, batch-acc = 1.0\n",
      "epoch = 72 step = 1020: train-loss = 0.10816165804862976, batch-acc = 1.0\n",
      "epoch = 72, val-acc = 0.8181818181818182\n",
      "Starting epoch 73 / 300\n",
      "epoch = 73 step = 1030: train-loss = 0.166888028383255, batch-acc = 1.0\n",
      "epoch = 73, val-acc = 0.8090909090909091\n",
      "Starting epoch 74 / 300\n",
      "epoch = 74 step = 1040: train-loss = 0.117630235850811, batch-acc = 1.0\n",
      "epoch = 74 step = 1050: train-loss = 0.19247795641422272, batch-acc = 1.0\n",
      "epoch = 74, val-acc = 0.8181818181818182\n",
      "Starting epoch 75 / 300\n",
      "epoch = 75 step = 1060: train-loss = 0.1421128511428833, batch-acc = 0.96875\n",
      "epoch = 75, val-acc = 0.8181818181818182\n",
      "Starting epoch 76 / 300\n",
      "epoch = 76 step = 1070: train-loss = 0.1545611172914505, batch-acc = 1.0\n",
      "epoch = 76, val-acc = 0.8727272727272727\n",
      "Starting epoch 77 / 300\n",
      "epoch = 77 step = 1080: train-loss = 0.14391684532165527, batch-acc = 1.0\n",
      "epoch = 77 step = 1090: train-loss = 0.1285477876663208, batch-acc = 1.0\n",
      "epoch = 77, val-acc = 0.7727272727272727\n",
      "Starting epoch 78 / 300\n",
      "epoch = 78 step = 1100: train-loss = 0.12814167141914368, batch-acc = 1.0\n",
      "epoch = 78, val-acc = 0.7636363636363637\n",
      "Starting epoch 79 / 300\n",
      "epoch = 79 step = 1110: train-loss = 0.1542542427778244, batch-acc = 0.96875\n",
      "epoch = 79 step = 1120: train-loss = 0.113705113530159, batch-acc = 1.0\n",
      "epoch = 79, val-acc = 0.7909090909090909\n",
      "Starting epoch 80 / 300\n",
      "epoch = 80 step = 1130: train-loss = 0.1191181167960167, batch-acc = 1.0\n",
      "epoch = 80, val-acc = 0.8090909090909091\n",
      "Starting epoch 81 / 300\n",
      "epoch = 81 step = 1140: train-loss = 0.1623334437608719, batch-acc = 1.0\n",
      "epoch = 81, val-acc = 0.7909090909090909\n",
      "Starting epoch 82 / 300\n",
      "epoch = 82 step = 1150: train-loss = 0.14414966106414795, batch-acc = 1.0\n",
      "epoch = 82 step = 1160: train-loss = 0.18668115139007568, batch-acc = 0.96875\n",
      "epoch = 82, val-acc = 0.7727272727272727\n",
      "Starting epoch 83 / 300\n",
      "epoch = 83 step = 1170: train-loss = 0.19462399184703827, batch-acc = 0.9375\n",
      "epoch = 83, val-acc = 0.7\n",
      "Starting epoch 84 / 300\n",
      "epoch = 84 step = 1180: train-loss = 0.12904717028141022, batch-acc = 0.96875\n",
      "epoch = 84 step = 1190: train-loss = 0.19147279858589172, batch-acc = 1.0\n",
      "epoch = 84, val-acc = 0.8181818181818182\n",
      "Starting epoch 85 / 300\n",
      "epoch = 85 step = 1200: train-loss = 0.1771271526813507, batch-acc = 0.96875\n",
      "epoch = 85, val-acc = 0.8\n",
      "Starting epoch 86 / 300\n",
      "epoch = 86 step = 1210: train-loss = 0.13064999878406525, batch-acc = 0.96875\n",
      "epoch = 86, val-acc = 0.7272727272727273\n",
      "Starting epoch 87 / 300\n",
      "epoch = 87 step = 1220: train-loss = 0.1557963490486145, batch-acc = 1.0\n",
      "epoch = 87 step = 1230: train-loss = 0.0911034494638443, batch-acc = 1.0\n",
      "epoch = 87, val-acc = 0.8\n",
      "Starting epoch 88 / 300\n",
      "epoch = 88 step = 1240: train-loss = 0.09597629308700562, batch-acc = 1.0\n",
      "epoch = 88, val-acc = 0.7818181818181819\n",
      "Starting epoch 89 / 300\n",
      "epoch = 89 step = 1250: train-loss = 0.12438948452472687, batch-acc = 1.0\n",
      "epoch = 89 step = 1260: train-loss = 0.18861554563045502, batch-acc = 0.95652174949646\n",
      "epoch = 89, val-acc = 0.8272727272727273\n",
      "Starting epoch 90 / 300\n",
      "epoch = 90 step = 1270: train-loss = 0.1558598279953003, batch-acc = 1.0\n",
      "epoch = 90, val-acc = 0.7636363636363637\n",
      "Starting epoch 91 / 300\n",
      "epoch = 91 step = 1280: train-loss = 0.1525045782327652, batch-acc = 1.0\n",
      "epoch = 91, val-acc = 0.8\n",
      "Starting epoch 92 / 300\n",
      "epoch = 92 step = 1290: train-loss = 0.09508484601974487, batch-acc = 1.0\n",
      "epoch = 92 step = 1300: train-loss = 0.11527340114116669, batch-acc = 1.0\n",
      "epoch = 92, val-acc = 0.7090909090909091\n",
      "Starting epoch 93 / 300\n",
      "epoch = 93 step = 1310: train-loss = 0.09408578276634216, batch-acc = 1.0\n",
      "epoch = 93, val-acc = 0.8272727272727273\n",
      "Starting epoch 94 / 300\n",
      "epoch = 94 step = 1320: train-loss = 0.09729056060314178, batch-acc = 1.0\n",
      "epoch = 94 step = 1330: train-loss = 0.12291224300861359, batch-acc = 1.0\n",
      "epoch = 94, val-acc = 0.7909090909090909\n",
      "Starting epoch 95 / 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 95 step = 1340: train-loss = 0.09118407964706421, batch-acc = 1.0\n",
      "epoch = 95, val-acc = 0.7\n",
      "Starting epoch 96 / 300\n",
      "epoch = 96 step = 1350: train-loss = 0.09959572553634644, batch-acc = 1.0\n",
      "epoch = 96, val-acc = 0.8\n",
      "Starting epoch 97 / 300\n",
      "epoch = 97 step = 1360: train-loss = 0.15722985565662384, batch-acc = 0.96875\n",
      "epoch = 97 step = 1370: train-loss = 0.09012533724308014, batch-acc = 1.0\n",
      "epoch = 97, val-acc = 0.8181818181818182\n",
      "Starting epoch 98 / 300\n",
      "epoch = 98 step = 1380: train-loss = 0.12572641670703888, batch-acc = 0.96875\n",
      "epoch = 98, val-acc = 0.7545454545454545\n",
      "Starting epoch 99 / 300\n",
      "epoch = 99 step = 1390: train-loss = 0.15660496056079865, batch-acc = 0.9375\n",
      "epoch = 99 step = 1400: train-loss = 0.08066313713788986, batch-acc = 1.0\n",
      "epoch = 99, val-acc = 0.7454545454545455\n",
      "Starting epoch 100 / 300\n",
      "epoch = 100 step = 1410: train-loss = 0.11099180579185486, batch-acc = 1.0\n",
      "epoch = 100, val-acc = 0.7818181818181819\n",
      "Starting epoch 101 / 300\n",
      "epoch = 101 step = 1420: train-loss = 0.0899551659822464, batch-acc = 1.0\n",
      "epoch = 101, val-acc = 0.7727272727272727\n",
      "Starting epoch 102 / 300\n",
      "epoch = 102 step = 1430: train-loss = 0.08319500088691711, batch-acc = 1.0\n",
      "epoch = 102 step = 1440: train-loss = 0.1396668255329132, batch-acc = 0.9375\n",
      "epoch = 102, val-acc = 0.8090909090909091\n",
      "Starting epoch 103 / 300\n",
      "epoch = 103 step = 1450: train-loss = 0.13981029391288757, batch-acc = 1.0\n",
      "epoch = 103, val-acc = 0.7909090909090909\n",
      "Starting epoch 104 / 300\n",
      "epoch = 104 step = 1460: train-loss = 0.11663752794265747, batch-acc = 1.0\n",
      "epoch = 104 step = 1470: train-loss = 0.25269076228141785, batch-acc = 0.9130434989929199\n",
      "epoch = 104, val-acc = 0.8090909090909091\n",
      "Starting epoch 105 / 300\n",
      "epoch = 105 step = 1480: train-loss = 0.08214244246482849, batch-acc = 1.0\n",
      "epoch = 105, val-acc = 0.7909090909090909\n",
      "Starting epoch 106 / 300\n",
      "epoch = 106 step = 1490: train-loss = 0.13924841582775116, batch-acc = 0.96875\n",
      "epoch = 106, val-acc = 0.7909090909090909\n",
      "Starting epoch 107 / 300\n",
      "epoch = 107 step = 1500: train-loss = 0.2130974680185318, batch-acc = 0.90625\n",
      "epoch = 107 step = 1510: train-loss = 0.0935528427362442, batch-acc = 1.0\n",
      "epoch = 107, val-acc = 0.6636363636363637\n",
      "Starting epoch 108 / 300\n",
      "epoch = 108 step = 1520: train-loss = 0.08852577209472656, batch-acc = 1.0\n",
      "epoch = 108, val-acc = 0.7818181818181819\n",
      "Starting epoch 109 / 300\n",
      "epoch = 109 step = 1530: train-loss = 0.08655934035778046, batch-acc = 1.0\n",
      "epoch = 109 step = 1540: train-loss = 0.0966835618019104, batch-acc = 1.0\n",
      "epoch = 109, val-acc = 0.7818181818181819\n",
      "Starting epoch 110 / 300\n",
      "epoch = 110 step = 1550: train-loss = 0.0913228914141655, batch-acc = 1.0\n",
      "epoch = 110, val-acc = 0.8363636363636363\n",
      "Starting epoch 111 / 300\n",
      "epoch = 111 step = 1560: train-loss = 0.08760389685630798, batch-acc = 1.0\n",
      "epoch = 111, val-acc = 0.7545454545454545\n",
      "Starting epoch 112 / 300\n",
      "epoch = 112 step = 1570: train-loss = 0.1164318174123764, batch-acc = 0.96875\n",
      "epoch = 112 step = 1580: train-loss = 0.10576657205820084, batch-acc = 1.0\n",
      "epoch = 112, val-acc = 0.8\n",
      "Starting epoch 113 / 300\n",
      "epoch = 113 step = 1590: train-loss = 0.07293223589658737, batch-acc = 1.0\n",
      "epoch = 113, val-acc = 0.8272727272727273\n",
      "Starting epoch 114 / 300\n",
      "epoch = 114 step = 1600: train-loss = 0.09686379134654999, batch-acc = 1.0\n",
      "epoch = 114 step = 1610: train-loss = 0.09727244824171066, batch-acc = 1.0\n",
      "epoch = 114, val-acc = 0.7545454545454545\n",
      "Starting epoch 115 / 300\n",
      "epoch = 115 step = 1620: train-loss = 0.0878414735198021, batch-acc = 1.0\n",
      "epoch = 115, val-acc = 0.7909090909090909\n",
      "Starting epoch 116 / 300\n",
      "epoch = 116 step = 1630: train-loss = 0.11241108924150467, batch-acc = 0.96875\n",
      "epoch = 116, val-acc = 0.8272727272727273\n",
      "Starting epoch 117 / 300\n",
      "epoch = 117 step = 1640: train-loss = 0.07652744650840759, batch-acc = 1.0\n",
      "epoch = 117 step = 1650: train-loss = 0.08300039172172546, batch-acc = 1.0\n",
      "epoch = 117, val-acc = 0.8545454545454545\n",
      "Starting epoch 118 / 300\n",
      "epoch = 118 step = 1660: train-loss = 0.09620007127523422, batch-acc = 1.0\n",
      "epoch = 118, val-acc = 0.8272727272727273\n",
      "Starting epoch 119 / 300\n",
      "epoch = 119 step = 1670: train-loss = 0.10253884643316269, batch-acc = 1.0\n",
      "epoch = 119 step = 1680: train-loss = 0.10633905231952667, batch-acc = 1.0\n",
      "epoch = 119, val-acc = 0.8272727272727273\n",
      "Starting epoch 120 / 300\n",
      "epoch = 120 step = 1690: train-loss = 0.08354179561138153, batch-acc = 1.0\n",
      "epoch = 120, val-acc = 0.8272727272727273\n",
      "Starting epoch 121 / 300\n",
      "epoch = 121 step = 1700: train-loss = 0.08728934079408646, batch-acc = 1.0\n",
      "epoch = 121, val-acc = 0.8363636363636363\n",
      "Starting epoch 122 / 300\n",
      "epoch = 122 step = 1710: train-loss = 0.08113329857587814, batch-acc = 1.0\n",
      "epoch = 122 step = 1720: train-loss = 0.13922324776649475, batch-acc = 0.96875\n",
      "epoch = 122, val-acc = 0.7363636363636363\n",
      "Starting epoch 123 / 300\n",
      "epoch = 123 step = 1730: train-loss = 0.056976042687892914, batch-acc = 1.0\n",
      "epoch = 123, val-acc = 0.8363636363636363\n",
      "Starting epoch 124 / 300\n",
      "epoch = 124 step = 1740: train-loss = 0.07115654647350311, batch-acc = 1.0\n",
      "epoch = 124 step = 1750: train-loss = 0.22223706543445587, batch-acc = 0.95652174949646\n",
      "epoch = 124, val-acc = 0.8090909090909091\n",
      "Starting epoch 125 / 300\n",
      "epoch = 125 step = 1760: train-loss = 0.0683852955698967, batch-acc = 1.0\n",
      "epoch = 125, val-acc = 0.8363636363636363\n",
      "Starting epoch 126 / 300\n",
      "epoch = 126 step = 1770: train-loss = 0.07820162922143936, batch-acc = 1.0\n",
      "epoch = 126, val-acc = 0.7545454545454545\n",
      "Starting epoch 127 / 300\n",
      "epoch = 127 step = 1780: train-loss = 0.12703046202659607, batch-acc = 0.96875\n",
      "epoch = 127 step = 1790: train-loss = 0.15314286947250366, batch-acc = 0.96875\n",
      "epoch = 127, val-acc = 0.7909090909090909\n",
      "Starting epoch 128 / 300\n",
      "epoch = 128 step = 1800: train-loss = 0.08249901235103607, batch-acc = 1.0\n",
      "epoch = 128, val-acc = 0.8727272727272727\n",
      "Starting epoch 129 / 300\n",
      "epoch = 129 step = 1810: train-loss = 0.08237645030021667, batch-acc = 1.0\n",
      "epoch = 129 step = 1820: train-loss = 0.07490174472332001, batch-acc = 1.0\n",
      "epoch = 129, val-acc = 0.8090909090909091\n",
      "Starting epoch 130 / 300\n",
      "epoch = 130 step = 1830: train-loss = 0.11040076613426208, batch-acc = 1.0\n",
      "epoch = 130, val-acc = 0.8272727272727273\n",
      "Starting epoch 131 / 300\n",
      "epoch = 131 step = 1840: train-loss = 0.07382024824619293, batch-acc = 1.0\n",
      "epoch = 131, val-acc = 0.8\n",
      "Starting epoch 132 / 300\n",
      "epoch = 132 step = 1850: train-loss = 0.07136761397123337, batch-acc = 1.0\n",
      "epoch = 132 step = 1860: train-loss = 0.16126924753189087, batch-acc = 0.96875\n",
      "epoch = 132, val-acc = 0.8\n",
      "Starting epoch 133 / 300\n",
      "epoch = 133 step = 1870: train-loss = 0.06299712508916855, batch-acc = 1.0\n",
      "epoch = 133, val-acc = 0.8181818181818182\n",
      "Starting epoch 134 / 300\n",
      "epoch = 134 step = 1880: train-loss = 0.08966457843780518, batch-acc = 1.0\n",
      "epoch = 134 step = 1890: train-loss = 0.07304838299751282, batch-acc = 1.0\n",
      "epoch = 134, val-acc = 0.8181818181818182\n",
      "Starting epoch 135 / 300\n",
      "epoch = 135 step = 1900: train-loss = 0.10295244306325912, batch-acc = 1.0\n",
      "epoch = 135, val-acc = 0.7909090909090909\n",
      "Starting epoch 136 / 300\n",
      "epoch = 136 step = 1910: train-loss = 0.07895518839359283, batch-acc = 1.0\n",
      "epoch = 136, val-acc = 0.8090909090909091\n",
      "Starting epoch 137 / 300\n",
      "epoch = 137 step = 1920: train-loss = 0.08812712132930756, batch-acc = 1.0\n",
      "epoch = 137 step = 1930: train-loss = 0.059695009142160416, batch-acc = 1.0\n",
      "epoch = 137, val-acc = 0.8181818181818182\n",
      "Starting epoch 138 / 300\n",
      "epoch = 138 step = 1940: train-loss = 0.06523364037275314, batch-acc = 1.0\n",
      "epoch = 138, val-acc = 0.8\n",
      "Starting epoch 139 / 300\n",
      "epoch = 139 step = 1950: train-loss = 0.055793412029743195, batch-acc = 1.0\n",
      "epoch = 139 step = 1960: train-loss = 0.09054245799779892, batch-acc = 1.0\n",
      "epoch = 139, val-acc = 0.8\n",
      "Starting epoch 140 / 300\n",
      "epoch = 140 step = 1970: train-loss = 0.05699315667152405, batch-acc = 1.0\n",
      "epoch = 140, val-acc = 0.8181818181818182\n",
      "Starting epoch 141 / 300\n",
      "epoch = 141 step = 1980: train-loss = 0.06414413452148438, batch-acc = 1.0\n",
      "epoch = 141, val-acc = 0.8272727272727273\n",
      "Starting epoch 142 / 300\n",
      "epoch = 142 step = 1990: train-loss = 0.053077880293130875, batch-acc = 1.0\n",
      "epoch = 142 step = 2000: train-loss = 0.07930682599544525, batch-acc = 1.0\n",
      "epoch = 142, val-acc = 0.8181818181818182\n",
      "Starting epoch 143 / 300\n",
      "epoch = 143 step = 2010: train-loss = 0.06140657514333725, batch-acc = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 143, val-acc = 0.8272727272727273\n",
      "Starting epoch 144 / 300\n",
      "epoch = 144 step = 2020: train-loss = 0.07589707523584366, batch-acc = 1.0\n",
      "epoch = 144 step = 2030: train-loss = 0.056630346924066544, batch-acc = 1.0\n",
      "epoch = 144, val-acc = 0.8545454545454545\n",
      "Starting epoch 145 / 300\n",
      "epoch = 145 step = 2040: train-loss = 0.06719879806041718, batch-acc = 1.0\n",
      "epoch = 145, val-acc = 0.8363636363636363\n",
      "Starting epoch 146 / 300\n",
      "epoch = 146 step = 2050: train-loss = 0.10326430946588516, batch-acc = 1.0\n",
      "epoch = 146, val-acc = 0.8272727272727273\n",
      "Starting epoch 147 / 300\n",
      "epoch = 147 step = 2060: train-loss = 0.12352955341339111, batch-acc = 1.0\n",
      "epoch = 147 step = 2070: train-loss = 0.055006686598062515, batch-acc = 1.0\n",
      "epoch = 147, val-acc = 0.8454545454545455\n",
      "Starting epoch 148 / 300\n",
      "epoch = 148 step = 2080: train-loss = 0.062332313507795334, batch-acc = 1.0\n",
      "epoch = 148, val-acc = 0.8636363636363636\n",
      "Starting epoch 149 / 300\n",
      "epoch = 149 step = 2090: train-loss = 0.08312539011240005, batch-acc = 1.0\n",
      "epoch = 149 step = 2100: train-loss = 0.07620727270841599, batch-acc = 1.0\n",
      "epoch = 149, val-acc = 0.8636363636363636\n",
      "Starting epoch 150 / 300\n",
      "epoch = 150 step = 2110: train-loss = 0.05602901056408882, batch-acc = 1.0\n",
      "epoch = 150, val-acc = 0.8545454545454545\n",
      "Starting epoch 151 / 300\n",
      "epoch = 151 step = 2120: train-loss = 0.1183205395936966, batch-acc = 1.0\n",
      "epoch = 151, val-acc = 0.7818181818181819\n",
      "Starting epoch 152 / 300\n",
      "epoch = 152 step = 2130: train-loss = 0.08538104593753815, batch-acc = 1.0\n",
      "epoch = 152 step = 2140: train-loss = 0.07691191881895065, batch-acc = 1.0\n",
      "epoch = 152, val-acc = 0.8\n",
      "Starting epoch 153 / 300\n",
      "epoch = 153 step = 2150: train-loss = 0.06373545527458191, batch-acc = 1.0\n",
      "epoch = 153, val-acc = 0.8181818181818182\n",
      "Starting epoch 154 / 300\n",
      "epoch = 154 step = 2160: train-loss = 0.06014532968401909, batch-acc = 1.0\n",
      "epoch = 154 step = 2170: train-loss = 0.05482818931341171, batch-acc = 1.0\n",
      "epoch = 154, val-acc = 0.8454545454545455\n",
      "Starting epoch 155 / 300\n",
      "epoch = 155 step = 2180: train-loss = 0.0478706881403923, batch-acc = 1.0\n",
      "epoch = 155, val-acc = 0.8363636363636363\n",
      "Starting epoch 156 / 300\n",
      "epoch = 156 step = 2190: train-loss = 0.058938756585121155, batch-acc = 1.0\n",
      "epoch = 156, val-acc = 0.8363636363636363\n",
      "Starting epoch 157 / 300\n",
      "epoch = 157 step = 2200: train-loss = 0.06910266727209091, batch-acc = 1.0\n",
      "epoch = 157 step = 2210: train-loss = 0.06926511973142624, batch-acc = 1.0\n",
      "epoch = 157, val-acc = 0.8454545454545455\n",
      "Starting epoch 158 / 300\n",
      "epoch = 158 step = 2220: train-loss = 0.06392747163772583, batch-acc = 1.0\n",
      "epoch = 158, val-acc = 0.8545454545454545\n",
      "Starting epoch 159 / 300\n",
      "epoch = 159 step = 2230: train-loss = 0.05134245753288269, batch-acc = 1.0\n",
      "epoch = 159 step = 2240: train-loss = 0.13725890219211578, batch-acc = 0.95652174949646\n",
      "epoch = 159, val-acc = 0.8272727272727273\n",
      "Starting epoch 160 / 300\n",
      "epoch = 160 step = 2250: train-loss = 0.05100718140602112, batch-acc = 1.0\n",
      "epoch = 160, val-acc = 0.8181818181818182\n",
      "Starting epoch 161 / 300\n",
      "epoch = 161 step = 2260: train-loss = 0.0837876945734024, batch-acc = 1.0\n",
      "epoch = 161, val-acc = 0.8454545454545455\n",
      "Starting epoch 162 / 300\n",
      "epoch = 162 step = 2270: train-loss = 0.07095260173082352, batch-acc = 1.0\n",
      "epoch = 162 step = 2280: train-loss = 0.05850963294506073, batch-acc = 1.0\n",
      "epoch = 162, val-acc = 0.8272727272727273\n",
      "Starting epoch 163 / 300\n",
      "epoch = 163 step = 2290: train-loss = 0.049551740288734436, batch-acc = 1.0\n",
      "epoch = 163, val-acc = 0.8818181818181818\n",
      "Starting epoch 164 / 300\n",
      "epoch = 164 step = 2300: train-loss = 0.060002975165843964, batch-acc = 1.0\n",
      "epoch = 164 step = 2310: train-loss = 0.05533580109477043, batch-acc = 1.0\n",
      "epoch = 164, val-acc = 0.8272727272727273\n",
      "Starting epoch 165 / 300\n",
      "epoch = 165 step = 2320: train-loss = 0.07355467975139618, batch-acc = 1.0\n",
      "epoch = 165, val-acc = 0.8454545454545455\n",
      "Starting epoch 166 / 300\n",
      "epoch = 166 step = 2330: train-loss = 0.048854656517505646, batch-acc = 1.0\n",
      "epoch = 166, val-acc = 0.8363636363636363\n",
      "Starting epoch 167 / 300\n",
      "epoch = 167 step = 2340: train-loss = 0.04366106912493706, batch-acc = 1.0\n",
      "epoch = 167 step = 2350: train-loss = 0.04752257838845253, batch-acc = 1.0\n",
      "epoch = 167, val-acc = 0.8363636363636363\n",
      "Starting epoch 168 / 300\n",
      "epoch = 168 step = 2360: train-loss = 0.05451292544603348, batch-acc = 1.0\n",
      "epoch = 168, val-acc = 0.7818181818181819\n",
      "Starting epoch 169 / 300\n",
      "epoch = 169 step = 2370: train-loss = 0.06360052525997162, batch-acc = 1.0\n",
      "epoch = 169 step = 2380: train-loss = 0.05085751414299011, batch-acc = 1.0\n",
      "epoch = 169, val-acc = 0.7181818181818181\n",
      "Starting epoch 170 / 300\n",
      "epoch = 170 step = 2390: train-loss = 0.10562711954116821, batch-acc = 0.96875\n",
      "epoch = 170, val-acc = 0.8454545454545455\n",
      "Starting epoch 171 / 300\n",
      "epoch = 171 step = 2400: train-loss = 0.10287562757730484, batch-acc = 0.96875\n",
      "epoch = 171, val-acc = 0.8545454545454545\n",
      "Starting epoch 172 / 300\n",
      "epoch = 172 step = 2410: train-loss = 0.05580664053559303, batch-acc = 1.0\n",
      "epoch = 172 step = 2420: train-loss = 0.12146653234958649, batch-acc = 0.9375\n",
      "epoch = 172, val-acc = 0.8636363636363636\n",
      "Starting epoch 173 / 300\n",
      "epoch = 173 step = 2430: train-loss = 0.05047871917486191, batch-acc = 1.0\n",
      "epoch = 173, val-acc = 0.8545454545454545\n",
      "Starting epoch 174 / 300\n",
      "epoch = 174 step = 2440: train-loss = 0.06304623186588287, batch-acc = 1.0\n",
      "epoch = 174 step = 2450: train-loss = 0.07478155940771103, batch-acc = 1.0\n",
      "epoch = 174, val-acc = 0.8454545454545455\n",
      "Starting epoch 175 / 300\n",
      "epoch = 175 step = 2460: train-loss = 0.05261782929301262, batch-acc = 1.0\n",
      "epoch = 175, val-acc = 0.8272727272727273\n",
      "Starting epoch 176 / 300\n",
      "epoch = 176 step = 2470: train-loss = 0.06336355954408646, batch-acc = 1.0\n",
      "epoch = 176, val-acc = 0.8272727272727273\n",
      "Starting epoch 177 / 300\n",
      "epoch = 177 step = 2480: train-loss = 0.06160367652773857, batch-acc = 1.0\n",
      "epoch = 177 step = 2490: train-loss = 0.06683536618947983, batch-acc = 1.0\n",
      "epoch = 177, val-acc = 0.8272727272727273\n",
      "Starting epoch 178 / 300\n",
      "epoch = 178 step = 2500: train-loss = 0.052234988659620285, batch-acc = 1.0\n",
      "epoch = 178, val-acc = 0.8363636363636363\n",
      "Starting epoch 179 / 300\n",
      "epoch = 179 step = 2510: train-loss = 0.055533744394779205, batch-acc = 1.0\n",
      "epoch = 179 step = 2520: train-loss = 0.05089268088340759, batch-acc = 1.0\n",
      "epoch = 179, val-acc = 0.8363636363636363\n",
      "Starting epoch 180 / 300\n",
      "epoch = 180 step = 2530: train-loss = 0.11095565557479858, batch-acc = 1.0\n",
      "epoch = 180, val-acc = 0.8727272727272727\n",
      "Starting epoch 181 / 300\n",
      "epoch = 181 step = 2540: train-loss = 0.049417026340961456, batch-acc = 1.0\n",
      "epoch = 181, val-acc = 0.8181818181818182\n",
      "Starting epoch 182 / 300\n",
      "epoch = 182 step = 2550: train-loss = 0.0503079891204834, batch-acc = 1.0\n",
      "epoch = 182 step = 2560: train-loss = 0.059439767152071, batch-acc = 1.0\n",
      "epoch = 182, val-acc = 0.8181818181818182\n",
      "Starting epoch 183 / 300\n",
      "epoch = 183 step = 2570: train-loss = 0.09712977707386017, batch-acc = 1.0\n",
      "epoch = 183, val-acc = 0.8181818181818182\n",
      "Starting epoch 184 / 300\n",
      "epoch = 184 step = 2580: train-loss = 0.037764087319374084, batch-acc = 1.0\n",
      "epoch = 184 step = 2590: train-loss = 0.19935651123523712, batch-acc = 0.95652174949646\n",
      "epoch = 184, val-acc = 0.8181818181818182\n",
      "Starting epoch 185 / 300\n",
      "epoch = 185 step = 2600: train-loss = 0.13040944933891296, batch-acc = 0.96875\n",
      "epoch = 185, val-acc = 0.7545454545454545\n",
      "Starting epoch 186 / 300\n",
      "epoch = 186 step = 2610: train-loss = 0.04149968922138214, batch-acc = 1.0\n",
      "epoch = 186, val-acc = 0.8090909090909091\n",
      "Starting epoch 187 / 300\n",
      "epoch = 187 step = 2620: train-loss = 0.05916702002286911, batch-acc = 1.0\n",
      "epoch = 187 step = 2630: train-loss = 0.04635562002658844, batch-acc = 1.0\n",
      "epoch = 187, val-acc = 0.8181818181818182\n",
      "Starting epoch 188 / 300\n",
      "epoch = 188 step = 2640: train-loss = 0.04832440987229347, batch-acc = 1.0\n",
      "epoch = 188, val-acc = 0.8\n",
      "Starting epoch 189 / 300\n",
      "epoch = 189 step = 2650: train-loss = 0.11755238473415375, batch-acc = 1.0\n",
      "epoch = 189 step = 2660: train-loss = 0.06228981167078018, batch-acc = 1.0\n",
      "epoch = 189, val-acc = 0.8\n",
      "Starting epoch 190 / 300\n",
      "epoch = 190 step = 2670: train-loss = 0.1015816405415535, batch-acc = 1.0\n",
      "epoch = 190, val-acc = 0.8181818181818182\n",
      "Starting epoch 191 / 300\n",
      "epoch = 191 step = 2680: train-loss = 0.04293131083250046, batch-acc = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 191, val-acc = 0.8181818181818182\n",
      "Starting epoch 192 / 300\n",
      "epoch = 192 step = 2690: train-loss = 0.051885850727558136, batch-acc = 1.0\n",
      "epoch = 192 step = 2700: train-loss = 0.05049509555101395, batch-acc = 1.0\n",
      "epoch = 192, val-acc = 0.8636363636363636\n",
      "Starting epoch 193 / 300\n",
      "epoch = 193 step = 2710: train-loss = 0.05257371440529823, batch-acc = 1.0\n",
      "epoch = 193, val-acc = 0.8363636363636363\n",
      "Starting epoch 194 / 300\n",
      "epoch = 194 step = 2720: train-loss = 0.054822176694869995, batch-acc = 1.0\n",
      "epoch = 194 step = 2730: train-loss = 0.08234139531850815, batch-acc = 1.0\n",
      "epoch = 194, val-acc = 0.8454545454545455\n",
      "Starting epoch 195 / 300\n",
      "epoch = 195 step = 2740: train-loss = 0.042406871914863586, batch-acc = 1.0\n",
      "epoch = 195, val-acc = 0.8181818181818182\n",
      "Starting epoch 196 / 300\n",
      "epoch = 196 step = 2750: train-loss = 0.09741330146789551, batch-acc = 1.0\n",
      "epoch = 196, val-acc = 0.8454545454545455\n",
      "Starting epoch 197 / 300\n",
      "epoch = 197 step = 2760: train-loss = 0.044609468430280685, batch-acc = 1.0\n",
      "epoch = 197 step = 2770: train-loss = 0.08256295323371887, batch-acc = 1.0\n",
      "epoch = 197, val-acc = 0.8454545454545455\n",
      "Starting epoch 198 / 300\n",
      "epoch = 198 step = 2780: train-loss = 0.06926345080137253, batch-acc = 1.0\n",
      "epoch = 198, val-acc = 0.8363636363636363\n",
      "Starting epoch 199 / 300\n",
      "epoch = 199 step = 2790: train-loss = 0.044734396040439606, batch-acc = 1.0\n",
      "epoch = 199 step = 2800: train-loss = 0.04284512624144554, batch-acc = 1.0\n",
      "epoch = 199, val-acc = 0.8272727272727273\n",
      "Starting epoch 200 / 300\n",
      "epoch = 200 step = 2810: train-loss = 0.037320081144571304, batch-acc = 1.0\n",
      "epoch = 200, val-acc = 0.8272727272727273\n",
      "Starting epoch 201 / 300\n",
      "epoch = 201 step = 2820: train-loss = 0.053460296243429184, batch-acc = 1.0\n",
      "epoch = 201, val-acc = 0.8272727272727273\n",
      "Starting epoch 202 / 300\n",
      "epoch = 202 step = 2830: train-loss = 0.04288078844547272, batch-acc = 1.0\n",
      "epoch = 202 step = 2840: train-loss = 0.04869136959314346, batch-acc = 1.0\n",
      "epoch = 202, val-acc = 0.7727272727272727\n",
      "Starting epoch 203 / 300\n",
      "epoch = 203 step = 2850: train-loss = 0.04309840500354767, batch-acc = 1.0\n",
      "epoch = 203, val-acc = 0.7545454545454545\n",
      "Starting epoch 204 / 300\n",
      "epoch = 204 step = 2860: train-loss = 0.05152522027492523, batch-acc = 1.0\n",
      "epoch = 204 step = 2870: train-loss = 0.04328108951449394, batch-acc = 1.0\n",
      "epoch = 204, val-acc = 0.8181818181818182\n",
      "Starting epoch 205 / 300\n",
      "epoch = 205 step = 2880: train-loss = 0.05026044324040413, batch-acc = 1.0\n",
      "epoch = 205, val-acc = 0.8090909090909091\n",
      "Starting epoch 206 / 300\n",
      "epoch = 206 step = 2890: train-loss = 0.04743190109729767, batch-acc = 1.0\n",
      "epoch = 206, val-acc = 0.8181818181818182\n",
      "Starting epoch 207 / 300\n",
      "epoch = 207 step = 2900: train-loss = 0.05959844961762428, batch-acc = 1.0\n",
      "epoch = 207 step = 2910: train-loss = 0.06839154660701752, batch-acc = 1.0\n",
      "epoch = 207, val-acc = 0.8363636363636363\n",
      "Starting epoch 208 / 300\n",
      "epoch = 208 step = 2920: train-loss = 0.057980023324489594, batch-acc = 1.0\n",
      "epoch = 208, val-acc = 0.8181818181818182\n",
      "Starting epoch 209 / 300\n",
      "epoch = 209 step = 2930: train-loss = 0.059341736137866974, batch-acc = 1.0\n",
      "epoch = 209 step = 2940: train-loss = 0.055945608764886856, batch-acc = 1.0\n",
      "epoch = 209, val-acc = 0.8454545454545455\n",
      "Starting epoch 210 / 300\n",
      "epoch = 210 step = 2950: train-loss = 0.05097757279872894, batch-acc = 1.0\n",
      "epoch = 210, val-acc = 0.8363636363636363\n",
      "Starting epoch 211 / 300\n",
      "epoch = 211 step = 2960: train-loss = 0.04036072641611099, batch-acc = 1.0\n",
      "epoch = 211, val-acc = 0.7909090909090909\n",
      "Starting epoch 212 / 300\n",
      "epoch = 212 step = 2970: train-loss = 0.05867389217019081, batch-acc = 1.0\n",
      "epoch = 212 step = 2980: train-loss = 0.0537322498857975, batch-acc = 1.0\n",
      "epoch = 212, val-acc = 0.7909090909090909\n",
      "Starting epoch 213 / 300\n",
      "epoch = 213 step = 2990: train-loss = 0.07648452371358871, batch-acc = 1.0\n",
      "epoch = 213, val-acc = 0.8272727272727273\n",
      "Starting epoch 214 / 300\n",
      "epoch = 214 step = 3000: train-loss = 0.04564579576253891, batch-acc = 1.0\n",
      "epoch = 214 step = 3010: train-loss = 0.06325258314609528, batch-acc = 1.0\n",
      "epoch = 214, val-acc = 0.8454545454545455\n",
      "Starting epoch 215 / 300\n",
      "epoch = 215 step = 3020: train-loss = 0.1419609785079956, batch-acc = 0.9375\n",
      "epoch = 215, val-acc = 0.8909090909090909\n",
      "Starting epoch 216 / 300\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0009986dd10b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0macc_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-204505a7921a>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Starting epoch %d / %d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    acc_list = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    plt.plot(acc_list)\n",
    "    plt.plot(acc_list, marker=\"*\", linewidth=3, linestyle=\"--\", color=\"orange\")\n",
    "    plt.ylabel('acc')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.title(\"model = {0}, base_lr = {1}, batch_size = {2}\".format(\"senet_3\", BASE_LR1, BATCH_SIZE))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
